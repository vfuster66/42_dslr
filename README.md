# ğŸ§™â€â™‚ï¸ dslr - Le Chapeau Magique de Poudlard en Python !

Bienvenue sur le projet **dslr** !  
Ce projet a pour but de prÃ©dire dans **quelle maison de Poudlard** seront rÃ©partis les Ã©lÃ¨ves, en recrÃ©ant le **Chapeau Magique** grÃ¢ce Ã  un **modÃ¨le de Machine Learning**, codÃ© **from scratch en Python**.

L'algorithme principal est une **rÃ©gression logistique multi-classe** utilisant la mÃ©thode **One-vs-All**. Ce projet suit une mÃ©thodologie complÃ¨te de **Data Science**, allant de la comprÃ©hension des donnÃ©es jusqu'Ã  l'Ã©valuation d'un modÃ¨le de classification.

ğŸª„ **Objectif :** prÃ©dire dans quelle maison seront rÃ©partis les nouveaux Ã©lÃ¨ves parmi :  
â¡ï¸ **Gryffondor**, **Poufsouffle**, **Serdaigle**, **Serpentard**.

---

## ğŸ§­ Contexte

Le **Chapeau Magique** de Poudlard est dÃ©fectueux ! Le professeur McGonagall fait appel Ã  vous, un(e) **data scientist moldu(e)**, pour crÃ©er un algorithme capable de trier les Ã©lÃ¨ves dans leur maison respective : **Gryffondor**, **Poufsouffle**, **Serdaigle** ou **Serpentard** .

Pour ce faire, vous utilisez les **donnÃ©es historiques** des Ã©lÃ¨ves : leurs notes dans diffÃ©rentes matiÃ¨res, leurs comportements, etc.

---

## ğŸ¯ Objectifs pÃ©dagogiques

Ce projet vise Ã  :
- **Comprendre et manipuler des donnÃ©es rÃ©elles**
- **ImplÃ©menter une rÃ©gression logistique multi-classe One-vs-All**
- **Apprendre Ã  entraÃ®ner un modÃ¨le sans librairie ML**
- **Atteindre une prÃ©cision d'au moins 98 % sur des donnÃ©es inconnues**

---

# âœ… Partie obligatoire - Ã‰tapes dÃ©taillÃ©es

Chaque Ã©tape est essentielle dans le pipeline d'un projet de Machine Learning :

---

## 1. ğŸ“‚ Analyse des donnÃ©es

### Script : `describe.py`

ğŸ” **Ce que Ã§a fait :**  
- **Charger** le dataset d'entraÃ®nement (`dataset_train.csv`).
- **Observer** la structure des donnÃ©es : colonnes, types de valeurs, valeurs manquantes.

### Concepts expliquÃ©s :
- **Count** : nombre de valeurs valides dans chaque colonne  
- **Mean (moyenne)** : la valeur moyenne de la colonne  
- **Standard Deviation (Ã©cart-type)** : mesure la dispersion des valeurs  
- **Min/Max** : valeur minimale et maximale  
- **Quartiles (25%, 50%, 75%)** : indiquent la distribution des donnÃ©es  
- **Skewness** : asymÃ©trie de la distribution (bonus)  
- **Kurtosis** : aplatissement ou concentration de la distribution (bonus)

### Pourquoi c'est important :
Avant de modÃ©liser quoi que ce soit, il faut **comprendre ses donnÃ©es**. Cette Ã©tape permet d'anticiper les problÃ¨mes (valeurs manquantes, colonnes inutiles, etc., donnÃ©es bien rÃ©parties et si certaines colonnes sont inutiles ou biaisÃ©es).

### ImplÃ©mentation : `app/describe.py`
- Calcul **manuel** de statistiques descriptives :
  - Nombre d'Ã©chantillons
  - Moyenne
  - Ã‰cart-type
  - Minimum, maximum
  - Quartiles (25%, 50%, 75%)

ğŸ”§ **Ã€ quoi Ã§a sert ?**
- Identifier les **variables discriminantes**.
- DÃ©tecter les **anomalies** (valeurs aberrantes, outliers).

#### Commande :
```bash
make describe
```

#### RÃ©sultats :
- Statistiques exportÃ©es en `.csv` et `.json` dans `data/describe/`.

---

## 2. ğŸ“Š Visualisation des donnÃ©es

La **visualisation** aide Ã  comprendre les **relations entre variables** et Ã  **choisir les features** pertinentes pour le modÃ¨le.

### 2.1. Histogrammes - `app/histogram.py`
ğŸ¨ **Ce que Ã§a fait :**  
- Affiche la distribution des scores par matiÃ¨re, maison par maison.
- Affiche un **histogramme** des notes par matiÃ¨re.
- Cherche les cours oÃ¹ la rÃ©partition est **homogÃ¨ne** entre les maisons.

#### Concept :
- **Histogramme** : montre comment les donnÃ©es sont rÃ©parties sur une Ã©chelle (valeurs frÃ©quentes ou rares).

ğŸ”§ **Pourquoi ?**
â¡ï¸ DÃ©terminer si une matiÃ¨re est **pertinente pour diffÃ©rencier les maisons**.

#### Commande :
```bash
make histogram
```

### 2.2. Scatter Plots - `app/scatter_plot.py`

ğŸ¨ **Ce que Ã§a fait :**
- Affiche des **nuages de points** entre deux variables.
- Cherche les deux features **les plus similaires** (corrÃ©lation).

#### Concept :
- **Scatter Plot** : montre la relation entre deux variables â†’ permet de voir si elles sont **corrÃ©lÃ©es**.

ğŸ”§ **Pourquoi ?**
â¡ï¸ Identifier les **variables similaires** pour Ã©viter la redondance dans l'entraÃ®nement du modÃ¨le.

#### Commande :
```bash
make scatter
```

### 2.3. Pair Plot - `app/pair_plot.py`
- Affiche une matrice complÃ¨te de Scatter Plots et d'histogrammes pour **toutes les paires de variables**.
- SÃ©lectionne les **features Ã  utiliser pour l'entraÃ®nement**.

ğŸ”§ **Pourquoi ?**
â¡ï¸ Pour **sÃ©lectionner les features** les plus pertinentes pour lâ€™entraÃ®nement.

#### Commande :
```bash
make pairplot
```

---

## 3. ğŸ¤– RÃ©gression Logistique (One-vs-All)

### Quâ€™est-ce que la **rÃ©gression logistique** ?  
â¡ï¸ Un algorithme **de classification** : il prÃ©dit une probabilitÃ© d'appartenance Ã  une classe.  
â¡ï¸ Ici, il sert Ã  prÃ©dire dans **quelle maison** ira l'Ã©lÃ¨ve.

---

### Quâ€™est-ce que le **One-vs-All** ?  
â¡ï¸ On transforme un problÃ¨me **multi-classe** en **plusieurs problÃ¨mes binaires** :  
  - Le modÃ¨le apprend Ã  dire "Maison Gryffondor : Oui ou Non ?", puis "Poufsouffle : Oui ou Non ?", etc.

---

### 3.1. EntraÃ®nement - `app/logreg_train.py`

ğŸ› ï¸ **Ce que Ã§a fait :**  
- EntraÃ®ne **un modÃ¨le par maison** via **descente de gradient**  
- Minimise l'erreur entre prÃ©diction et rÃ©alitÃ©  
- **RÃ©gularisation L2** : limite lâ€™amplitude des poids pour Ã©viter le **sur-apprentissage**
- Calcul des **poids** pour chaque classe.
- Sauvegarde des poids dans `data/logreg_model.npy`.

### Concepts expliquÃ©s :
- **Descente de gradient** : algorithme qui ajuste progressivement les poids pour **minimiser l'erreur**.  
- **RÃ©gularisation L2** : pÃ©nalise les poids trop grands pour **amÃ©liorer la gÃ©nÃ©ralisation**.

ğŸ”§ **Pourquoi ?**
- La descente de gradient ajuste les **paramÃ¨tres** pour minimiser l'erreur entre les **prÃ©dictions** et la **rÃ©alitÃ©**.

#### Commande :
```bash
make train
```

### 3.2. PrÃ©diction - `app/logreg_predict.py`

ğŸ› ï¸ **Ce que Ã§a fait :**  
- Charge le modÃ¨le entraÃ®nÃ©  
- PrÃ©dit la **maison** de chaque Ã©lÃ¨ve du `dataset_test.csv`

ğŸ”§ **Pourquoi ?**
- GÃ©nÃ©rer des **prÃ©dictions** quâ€™on pourra **Ã©valuer** ensuite.

#### Commande :
```bash
make predict
```

---

## 4. ğŸ§® Ã‰valuation du modÃ¨le

ğŸ“ˆ **Ce que Ã§a fait :**  
- Calcule la **prÃ©cision globale** (% de bonnes rÃ©ponses)  
- CrÃ©e une **matrice de confusion** : montre oÃ¹ se trouvent les erreurs  
- GÃ©nÃ¨re un **rapport de classification** : prÃ©cision, rappel, F1-score

ğŸ”§ **Pourquoi ?**
- Ã‰valuer objectivement les **performances** du modÃ¨le.
- Voir si le modÃ¨le est **Ã©quilibrÃ©** entre les diffÃ©rentes classes.

#### Commande :
```bash
make evaluate
```

---

# ğŸ Partie bonus - FonctionnalitÃ©s avancÃ©es

---

## 1. ğŸ”¹ SÃ©lection manuelle des meilleures features

### Pourquoi ?
- RÃ©duire la **dimensionnalitÃ©** des donnÃ©es pour Ã©viter le **surapprentissage**.

### Ce quâ€™on a fait :
- Analyse des **pair plots** pour repÃ©rer les features **les plus discriminantes**.
- Stockage des meilleures features dans `data/best_features.txt`.

#### Commande :
```bash
make pairplot
```

---

## 2. ğŸ”¹ Statistiques avancÃ©es dans `describe.py`

### Pourquoi ?
- Comprendre plus finement la **distribution** des variables.

### Ce quâ€™on a ajoutÃ© :
- **Skewness** : asymÃ©trie de la distribution.
- **Kurtosis** : aplatissement de la distribution.

#### Commande :
```bash
make describe
```

---

## 3. ğŸ”¹ Courbes ROC dÃ©taillÃ©es et Super ROC

### Pourquoi ?
- Visualiser les performances de chaque classifieur **One-vs-All**.

### Ce quâ€™on a ajoutÃ© :
- Courbes ROC pour chaque maison.
- Super ROC : toutes les courbes affichÃ©es sur le mÃªme graphe.

#### Commande :
```bash
make roc
make roc-all
```

---

## 4. ğŸ”¹ Mini-Batch Gradient Descent (MBGD)

### Pourquoi ?
- AccÃ©lÃ©rer la **descente de gradient** sur des jeux de donnÃ©es plus volumineux.

### Ce quâ€™on a fait :
- ImplÃ©mentation d'une **descente en mini-lots** dans `logreg_train.py`.
- Gestion de la **taille des batchs** paramÃ©trable (par dÃ©faut 32).

#### Commande :
```bash
make train-mbgd
```

---

## ğŸ³ Docker & Makefile intÃ©grÃ©s

### Pourquoi ?
- Simplifier l'exÃ©cution des scripts.
- Garantir la **reproductibilitÃ©** sur tout environnement.

### Ce quâ€™on a :
- `Dockerfile` : environnement Python 3.10 avec dÃ©pendances.
- `Makefile` : automatisation des commandes Docker.

#### Commande :
Principales commandes :
```bash
make build
make shell
make describe
make train
```

---

## ğŸ“š Ressources complÃ©mentaires

- [Coursera - Machine Learning par Andrew Ng](https://www.coursera.org/learn/machine-learning)
- [Khan Academy - ProbabilitÃ©s et Statistiques](https://fr.khanacademy.org/math/statistics-probability)
- [Matplotlib](https://matplotlib.org/)
- [Seaborn](https://seaborn.pydata.org/)
- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)

---

