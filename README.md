# ğŸ§™â€â™‚ï¸ dslr - Le Chapeau Magique de Poudlard en Python !

Bienvenue sur le projet **dslr** !  
Ce projet a pour but de prÃ©dire dans **quelle maison de Poudlard** seront rÃ©partis les Ã©lÃ¨ves, en recrÃ©ant le **Chapeau Magique** grÃ¢ce Ã  un **modÃ¨le de Machine Learning**, codÃ© **from scratch en Python**.

L'algorithme principal est une **rÃ©gression logistique multi-classe** utilisant la mÃ©thode **One-vs-All**. Ce projet suit une mÃ©thodologie complÃ¨te de **Data Science**, allant de la comprÃ©hension des donnÃ©es jusqu'Ã  l'Ã©valuation d'un modÃ¨le de classification.

---

## ğŸ§­ Contexte

Le **Chapeau Magique** de Poudlard est dÃ©fectueux ! Le professeur McGonagall fait appel Ã  vous, un(e) **data scientist moldu(e)**, pour crÃ©er un algorithme capable de trier les Ã©lÃ¨ves dans leur maison respective : **Gryffondor**, **Poufsouffle**, **Serdaigle** ou **Serpentard**.

Pour ce faire, vous utilisez les **donnÃ©es historiques** des Ã©lÃ¨ves : leurs notes dans diffÃ©rentes matiÃ¨res, leurs comportements, etc.

---

## ğŸ¯ Objectifs pÃ©dagogiques

Ce projet vise Ã  :
- ImplÃ©menter une **rÃ©gression logistique multi-classe** sans bibliothÃ¨que de Machine Learning haut niveau.
- Apprendre Ã  **analyser**, **nettoyer**, **visualiser** et **modÃ©liser** des donnÃ©es.
- Comprendre le cycle complet d'un projet de **classification supervisÃ©e**.

---

# âœ… Partie obligatoire - Ã‰tapes dÃ©taillÃ©es

Chaque Ã©tape est essentielle dans le pipeline d'un projet de Machine Learning :

---

## 1. ğŸ“‚ Analyse des donnÃ©es

### Ce quâ€™on fait :
- **Charger** le dataset d'entraÃ®nement (`dataset_train.csv`).
- **Observer** la structure des donnÃ©es : colonnes, types de valeurs, valeurs manquantes.

### Pourquoi c'est important :
Avant de modÃ©liser quoi que ce soit, il faut **comprendre ses donnÃ©es**. Cette Ã©tape permet d'anticiper les problÃ¨mes (valeurs manquantes, colonnes inutiles, etc.).

### ImplÃ©mentation : `app/describe.py`
- Calcul **manuel** de statistiques descriptives :
  - Nombre d'Ã©chantillons
  - Moyenne
  - Ã‰cart-type
  - Minimum, maximum
  - Quartiles (25%, 50%, 75%)

ğŸ”§ **Ã€ quoi Ã§a sert ?**
- Identifier les **variables discriminantes**.
- DÃ©tecter les **anomalies** (valeurs aberrantes, outliers).

#### Exemple d'utilisation :
```bash
make describe
```

#### RÃ©sultats :
- Statistiques exportÃ©es en `.csv` et `.json` dans `data/describe/`.

---

## 2. ğŸ“Š Visualisation des donnÃ©es

La **visualisation** aide Ã  comprendre les **relations entre variables** et Ã  **choisir les features** pertinentes pour le modÃ¨le.

### 2.1. Histogrammes - `app/histogram.py`
- Affiche un **histogramme** des notes par matiÃ¨re.
- Cherche les cours oÃ¹ la rÃ©partition est **homogÃ¨ne** entre les maisons.

ğŸ”§ **Pourquoi ?**
- Permet de dÃ©tecter si certaines matiÃ¨res sont **plus ou moins discriminantes**.

```bash
make histogram
```

### 2.2. Scatter Plots - `app/scatter_plot.py`
- Affiche des **nuages de points** entre deux variables.
- Cherche les deux features **les plus similaires** (corrÃ©lation).

ğŸ”§ **Pourquoi ?**
- Comprendre quelles features sont **corrÃ©lÃ©es**, et ainsi Ã©viter la **redondance** dans les variables.

```bash
make scatter
```

### 2.3. Pair Plot - `app/pair_plot.py`
- GÃ©nÃ¨re une **matrice de scatter plots** pour observer toutes les relations.
- SÃ©lectionne les **features Ã  utiliser pour l'entraÃ®nement**.

ğŸ”§ **Pourquoi ?**
- Aide Ã  faire une **sÃ©lection de features** basÃ©e sur l'observation visuelle des distributions et relations.

```bash
make pairplot
```

---

## 3. ğŸ¤– RÃ©gression Logistique (One-vs-All)

### Ce quâ€™on fait :
- ImplÃ©menter un **classifieur multi-classe** en One-vs-All avec **rÃ©gression logistique**.
- Chaque maison devient une **classe**, et le modÃ¨le apprend Ã  **prÃ©dire la probabilitÃ©** qu'un Ã©lÃ¨ve appartienne Ã  cette maison.

### 3.1. EntraÃ®nement - `app/logreg_train.py`
- ImplÃ©mentation de la **descente de gradient** classique.
- Calcul des **poids** pour chaque classe.
- Sauvegarde des poids dans `data/logreg_model.npy`.

ğŸ”§ **Pourquoi ?**
- La descente de gradient ajuste les **paramÃ¨tres** pour minimiser l'erreur entre les **prÃ©dictions** et la **rÃ©alitÃ©**.

```bash
make train
```

### 3.2. PrÃ©diction - `app/logreg_predict.py`
- Utilise les poids appris pour **prÃ©dire** la maison de chaque Ã©lÃ¨ve du `dataset_test.csv`.
- Produit le fichier `houses.csv`.

ğŸ”§ **Pourquoi ?**
- GÃ©nÃ©rer des **prÃ©dictions** quâ€™on pourra **Ã©valuer** ensuite.

```bash
make predict
```

---

## 4. ğŸ§® Ã‰valuation du modÃ¨le

### Ce quâ€™on fait :
- Ã‰valuer la qualitÃ© de nos **prÃ©dictions** sur les donnÃ©es de test.

### ImplÃ©mentation - `app/logreg_evaluate.py`
- GÃ©nÃ©ration de **matrices de confusion**, **rapports de classification** et **courbes ROC** pour chaque maison.
- VÃ©rification que le modÃ¨le atteint **98% de prÃ©cision** (critÃ¨re du sujet).

ğŸ”§ **Pourquoi ?**
- Ã‰valuer objectivement les **performances** du modÃ¨le.
- Voir si le modÃ¨le est **Ã©quilibrÃ©** entre les diffÃ©rentes classes.

```bash
make evaluate
```

---

# ğŸ Partie bonus - FonctionnalitÃ©s avancÃ©es

---

## 1. ğŸ”¹ SÃ©lection manuelle des meilleures features

### Pourquoi ?
- RÃ©duire la **dimensionnalitÃ©** des donnÃ©es pour Ã©viter le **surapprentissage**.

### Ce quâ€™on a fait :
- Analyse des **pair plots** pour repÃ©rer les features **les plus discriminantes**.
- Stockage des meilleures features dans `data/best_features.txt`.

```bash
make pairplot
```

---

## 2. ğŸ”¹ Statistiques avancÃ©es dans `describe.py`

### Pourquoi ?
- Comprendre plus finement la **distribution** des variables.

### Ce quâ€™on a ajoutÃ© :
- **Skewness** : asymÃ©trie de la distribution.
- **Kurtosis** : aplatissement de la distribution.

```bash
make describe
```

---

## 3. ğŸ”¹ Courbes ROC dÃ©taillÃ©es et Super ROC

### Pourquoi ?
- Visualiser les performances de chaque classifieur **One-vs-All**.

### Ce quâ€™on a ajoutÃ© :
- Courbes ROC pour chaque maison.
- Super ROC : toutes les courbes affichÃ©es sur le mÃªme graphe.

```bash
make roc
make roc-all
```

---

## 4. ğŸ”¹ Mini-Batch Gradient Descent (MBGD)

### Pourquoi ?
- AccÃ©lÃ©rer la **descente de gradient** sur des jeux de donnÃ©es plus volumineux.

### Ce quâ€™on a fait :
- ImplÃ©mentation d'une **descente en mini-lots** dans `logreg_train.py`.
- Gestion de la **taille des batchs** paramÃ©trable (par dÃ©faut 32).

```bash
make train-mbgd
```

---

## ğŸ³ Docker & Makefile intÃ©grÃ©s

### Pourquoi ?
- Simplifier l'exÃ©cution des scripts.
- Garantir la **reproductibilitÃ©** sur tout environnement.

### Ce quâ€™on a :
- `Dockerfile` : environnement Python 3.10 avec dÃ©pendances.
- `Makefile` : automatisation des commandes Docker.

Principales commandes :
```bash
make build
make shell
make describe
make train
```

---

## ğŸ“š Ressources complÃ©mentaires

- [Coursera - Machine Learning par Andrew Ng](https://www.coursera.org/learn/machine-learning)
- [Khan Academy - ProbabilitÃ©s et Statistiques](https://fr.khanacademy.org/math/statistics-probability)
- [Matplotlib](https://matplotlib.org/)
- [Seaborn](https://seaborn.pydata.org/)
- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)

---

## ğŸ™Œ Auteurs

Projet rÃ©alisÃ© dans le cadre de l'Ã©cole **42**, module **dslr**.

- Auteur : [Ton nom ici]
- AnnÃ©e : 2024

---

